\subsubsection{Q1}

\begin{verbatim}
int x = 10, y = 0;
co
    while (x!=y) { ##A
        x = x-1; ##B
    }
    y = y+1; ##C
    //
    <await (x==y);> ##D
    x = 8; ##E
    y = 2; ##F
oc
\end{verbatim}

Investigating the program we find that there are the following paths
through the program.

Path are represented in a simalar way to regex, in that (A,B)* means 0
or more iterations of A followed by B Where a line isn't an atomic
operation, C represents all operations on the line but C' is the
operation, C'' the second operation etc

In many cases there are a large number of possible paths to the output.
While I have hopefully listed all possible outputs the paths given are
just an example of how to reach that output

First considering only simple paths through the program that treat the
loop (A,B) and all lines as atomic we find the following ouputs and
paths:

\begin{verbatim}
x = 8, y = 3: (A,B)*, D, E, F, C
x = 8, y = 2: (A,B)*, D, E, C, F

x = 0, y = 3: (A,B)*, D, E, (A,B)*, F, C
x = 0, y = 2: (A,B)*, D, E, (A,B)*, C, F

x = 0, y = 1 (fail to terminate): (A, B)*, C, D
\end{verbatim}

The result where x = 0, y = 1 fails to terminate due to the condition
for line D not being able to be met.

Lines C and B are clearly not atomic as they are actually both a read
and a write. Representing the operation in some form of asmembly languge
may give something such as follows:

\begin{verbatim}
Load @x, r1
Add r1, 1
Store r1, @x
\end{verbatim}

Clearly other commands can be interleaved between these three commands,
leading to inconsistencies, so we cannot consider just a single thread.

The add is not observable to other processors, so in our considerations
we can reduce the complexity of our considerations by just considering
the observable events such for B and C, i.e.~load and store. We labeled
these as B' and B'`or C' and C'' for B and C respectively.

\begin{verbatim}
Load @x, r1 ##B'
Add r1, 1
Store r1, @x ##B''
\end{verbatim}

This means that other operations on the variable can be interleaved
between the read and the write. As mentioned above the notation used is
that the operation C is in fact two operations, C and C'. Considering
this along

\begin{verbatim}
x = 8, y = 1: (A, B)*, D, E, C', F, C''
x = 0, y = 1: (A, B)*, B, D, E, (A,B)*, C', F, C''
\end{verbatim}

There is also the case when line F sets y to a value greater than x
causing an infinite loop. Depending on the model there is of course the
chance that x will underflow but I assume that we are dealing with a
language that has integers that are not bounded by word length.

\begin{verbatim}
x -> -INF , y = 2: (A, B)*, D, E, (A, B)*, F, (A, B)*, C
\end{verbatim}

The following examples are very dependant on the points at which the
interleaving happens. For example if at the point at the point F is
executed x is less than 2 then the program will fail to terminate with x
tending to minus infinity. This leads to the same set of answers as one
of the above paths but just uses a slightly more complex path to reach
the result

\begin{verbatim}
x = 2, y = 3: (A, B)*, D, E, (A, B)*, A, F, B, (A, B)*, C 
\end{verbatim}

Of course this is not the only set of paths. The code we were
considering was unoptimized and so maintained its order. Due to things
such as super scaler execution and reordering, loop unrolling etc. I
would suspect that the actual possible outputs are far larger and also
compiler and machine dependant. I also expect that this would be close
to impossible to analysis by hand.

I couldn't find any cases where the lack of atomicity in lines A (due to
two loads) and B caused a difference in output.

\subsubsection{Q2}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  In the mp impl, messages sent are deg(v) and rndvalue(v) and the first
  legal colour
\item
  In the shared memory version this data is readable by everyone
\end{itemize}

In the message passing implementation of the algorithm each processor
represents a node in the graph. Each processor holds internal state
about the node $v$ such as the list of colours it that it is impossible
for the node to be. At each iteration of the algorithm if the node
hasn't yet been allocated a colour the processor sends deg(v),
rndvalue(v) and its choice of colour to all its neighbours and preforms
analysis on that data.

This presented algorithm multiple iterations, where each iteration is
made up of a defined number stages. Unlike the message passing
implementation all data is readable by all nodes which means that rather
than nodes sending data to receiving nodes, the receiving nodes would
just read the data themselves. This leads to the problem that it is
possible for the receiving node to read data in an inconsistent state
and or if the data is processed at different speeds the algorithm could
move out of lockstep and so one node could be several iterations ahead
of other nodes thereby breaking the algorithm.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  This is most simalar to the Interacting Peers Pattern

  \begin{itemize}
  \item
    Same Program Multiple Data
  \item
    Each processor responsible for maintaining data about invlaid
    colours etc
  \item
    Use barriers to maintain syncronisation. Simalar to Jacboi
    algo\ldots{} mentioned in lectures
  \item
    Dealing with more tasks per node just means that a single processor
    has to preform the work of multiple nodes per round
  \item
    Sequentially
  \item
    Either threads to allow each task to be run in parallel
  \item
    Equlivent as long as long as the entire thing remains in lockstep
  \end{itemize}
\end{itemize}

This is most similar to the interacting peers pattern, as each processor
has some other number of processors it interacts with defined as defined
by the graph. As with other iteration based algorithms that use this
pattern such as the Jacobi algorithm an ideal way of synchronising the
processors is to use barriers to ensure the algorithm runs in lockstep
and every processor remains at the same stage and iteration of the
algorithm as every other processor. This barrier would ensure that all
processors wait until all active processors have entered the barrier,
but given that it is possible for a node to be allocated a colour and so
not need to preform any further processing care should be taken that
only active processors need to enter the barrier.

If the algorithm was altered to allow for multiple graph nodes per
processor then for each stage of the algorithm each processor would have
to preform each the tasks for some number of nodes \textgreater{}= 1.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Could be implemented using bag of tasks, but doesn't fit that well

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Would need to use a master to schedule tasks
  \item
    Does have the advantage of implictly supporting more than one node
    per processor
  \item
    Allows different performance of nodes as faster nodes just process
    more tasks
  \item
    load distribution, deals better with nodes stopping
  \item
    node not tied to a processor
  \end{itemize}
\end{itemize}

It would also be possible to implement this use the bag of tasks
pattern. This isn't quite as elegant and it would be slightly shoehorned
in however it does allow have several distinct advantages, most notably
the ability lack of a processor being tied to a set of nodes (which is
possible due to shared memory between processors) and the ability to
load balance nodes between processors to minimize the waiting time if a
processor is idle (due to all its nodes having been assigned a colour).

Care would have to be taken to ensure that the task queue is ordered so
that the tasks are processed in the correct order. Any system implanting
this would have to be taken to ensure that the tasks don't overrun and
tasks for a further iteration started before all the tasks for the
current iteration have finished. This could be implemented by a bag of
tasks that has some understanding of the task and so requires all
processors to be waiting for a task before the next iterations tasks are
made available.

Using this method does mean that having multiple nodes per processor is
trivial as the pattern doesn't make any assumption of which node belongs
to which processor and so to support more nodes than processors is
simply a matter of creating more tasks.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  In this case, neither the other two patterns metnioned in lectures
  makes any sense

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Producer and consumer
  \item
    Pipeline
  \end{itemize}
\item
  Not the best of ideas, maybe use a better algo? Look into this
\end{itemize}
